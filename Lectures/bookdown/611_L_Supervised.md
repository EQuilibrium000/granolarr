---
title: "Lecture 611<br/>Supervised ML"
author: "Dr Stefano De Sabbata<br/>School of Geography, Geology, and the Env.<br/><a href=\"mailto:s.desabbata@le.ac.uk\">s.desabbata&commat;le.ac.uk</a> &vert; <a href=\"https://twitter.com/maps4thought\">&commat;maps4thought</a><br/><a href=\"https://github.com/sdesabbata/GY7702\">github.com/sdesabbata/GY7702</a> licensed under <a href=\"https://www.gnu.org/licenses/gpl-3.0.html\">GNU GPL v3.0</a>"
date: "2019-11-18"
output:
  ioslides_presentation:
    template: ../Utils/IOSlides/UoL_Template.html
    logo: ../Utils/IOSlides/uol_logo.png
---





# Recap @ 611



## Previous lectures

TODO



## Today

TODO



# k Nearest Neighbors



## Classification task



## k Nearest Neighbors



## Choosing k



# Neural networks



## Artificial Neural Networks



## Artificial neurons

<!--
Note from Machine Learning with R: Expert techniques for predictive modeling by Brett Lantz, Packt Publishing, 2019
- An activation function, which transforms a neuron's combined input signals into a single output signal to be broadcasted further in the network
- A network topology (or architecture), which describes the number of neurons in the model as well as the number of layers and manner in which they are connected
- The training algorithm that specifies how connection weights are set in order to inhibit or excite neurons in proportion to the input signal
-->




## Activation functions



## Network topology



## Deep Neural Networks



## Recurrent Neural Networks



## Backpropagation



## Training



## Evaluation




# Support Vector Machines



## Support Vector Machines

<!--
Note from Machine Learning with R: Expert techniques for predictive modeling by Brett Lantz, Packt Publishing, 2019
the SVM learning combines aspects of both the instance-based nearest neighbor learning presented in Chapter 3, Lazy Learning – Classification Using Nearest Neighbors, and the linear regression modeling described in Chapter 6, Forecasting Numeric Data – Regression Methods. 
-->


## Maximum Margin Hyperplane



## Support Vectors



## Linearly separable



## Non-linearly separable



## Kernels



# Summary



## Summary

TODO



## Practical session

In the practical session we will see:

TODO
